{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Scraper File\n",
    "Before realizing that Glottolog provides a link to the relevant Wikipedia page for a given language, I began writing my web scraper to obtain information from the language pages listed in Wikipedia's https://en.wikipedia.org/wiki/List_of_language_names page. This page provides a list of commonly known languages and their links, but does not provide pages for a complete list of languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links to the pages for each language, scraped from Wikipedia:\n",
    "    # def get_links(): \n",
    "    #     response = requests.get('https://en.wikipedia.org/wiki/List_of_language_names')\n",
    "    #     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #     links = soup.find_all('a')\n",
    "    #     # links = [link.get('href') for link in links if (link.get('title') is not None) and ('language' in link.get('title'))]\n",
    "    #     links = [link.get('href') for link in links if (link.get('title') is not None)]\n",
    "    #     # links = [link for link in links if link.endswith('_language')]\n",
    "    #     links = [\"https://en.wikipedia.org\"+link for link in links]\n",
    "    #     return links\n",
    "\n",
    "#Updated Solution: Links to the pages for each language, scraped from Glottolog: \n",
    "glottolog_info = pd.read_csv('../csv_files/glottolog_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function finds the index of the headers in the infobox of the Wikipedia page for a given language, allowing \n",
    "# us to extract the information we need from the page using the headers as keys.\n",
    "\n",
    "def find_index_of_headers(url):\n",
    "    response        = requests.get(url)\n",
    "    content         = response.content\n",
    "    parser          = BeautifulSoup(content, 'html.parser')\n",
    "    table           = parser.find('table', {'class': 'infobox'})\n",
    "    included_headers = {}\n",
    "    \n",
    "    for i in range(0, len(table.find_all('th', {'class': 'infobox-label'}))):\n",
    "        included_headers[unicodedata.normalize('NFKD', table.find_all('th', {'class': 'infobox-label'}).__getitem__(i).getText())] = i\n",
    "    return included_headers\n",
    "\n",
    "\n",
    "urls = glottolog_info['Wikipedia_Url'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code block scrapes the Wikipedia pages for each language in the list of links, extracting the information\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "data = []\n",
    "errors = []\n",
    "\n",
    "for url in tqdm(urls): \n",
    "    # try: \n",
    "        # lang = url.split('/')[-1]\n",
    "\n",
    "        # print(requests.get(url).status_code)\n",
    "        # print(url)\n",
    "\n",
    "        session = requests.Session()\n",
    "        retry = Retry(connect=3, backoff_factor=0.5)\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "\n",
    "        if url == 'nan':\n",
    "             continue\n",
    "        if str(url).startswith('/wiki/'):\n",
    "             url = 'https://en.wikipedia.org' + str(url)\n",
    "        if requests.get(str(url)).status_code != 200:\n",
    "             continue\n",
    "        response = session.get(str(url))\n",
    "\n",
    "        content         = response.content\n",
    "        parser          = BeautifulSoup(content, 'html.parser')\n",
    "        table           = parser.find('table', {'class': 'infobox'})\n",
    "\n",
    "        if table == None:\n",
    "             print(\"No table found \", url)\n",
    "             continue\n",
    "\n",
    "        header_indices  = find_index_of_headers(url)\n",
    "\n",
    "        # print(header_indices)\n",
    "\n",
    "        infobox_data = table.find_all('td', {'class': 'infobox-data'})\n",
    "\n",
    "        lang            = table.find('th', {'class': 'infobox-above above'}).get_text('title') if table.find('th', {'class': 'infobox-above above'}) else lang\n",
    "        off_lang        = infobox_data[header_indices['Official language in']].text if 'Official language in' in header_indices else None\n",
    "        rec_min_lang    = infobox_data[header_indices['Recognised minoritylanguage in']].text if 'Recognised minoritylanguage in' in header_indices else None\n",
    "        # speakers        = infobox_data[header_indices['Speakers']].text             if 'Speakers' in header_indices else None\n",
    "        iso6393        = infobox_data[header_indices['ISO 639-3']].get_text('title')            if 'ISO 639-3' in header_indices else None\n",
    "        glottocode      = infobox_data[header_indices['Glottolog']].get_text('title')            if 'Glottolog' in header_indices else None\n",
    "\n",
    "        if 'Speakers' in header_indices.keys():\n",
    "            speakers = infobox_data[header_indices['Speakers']].text\n",
    "        elif 'Native speakers' in header_indices.keys():\n",
    "            speakers = infobox_data[header_indices['Native speakers']].text\n",
    "        else:\n",
    "            speakers = None\n",
    "\n",
    "        if 'Region' in header_indices.keys():\n",
    "            regions = infobox_data[header_indices['Region']].text\n",
    "        elif 'Native Region' in header_indices.keys():\n",
    "            regions = infobox_data[header_indices['Native Region']].text\n",
    "        else:\n",
    "            regions = None\n",
    "\n",
    "        family          = [x for x in infobox_data[header_indices['Language family']].get_text('title').split('title') if x != \"\\n\"]\\\n",
    "                            if 'Language family' in header_indices else None\n",
    "        \n",
    "        dialects        = [x for x in infobox_data[header_indices['Dialects']].get_text('title').split('title') if x != \"\\n\"] \\\n",
    "                            if 'Dialects' in header_indices else None\n",
    "\n",
    "        url             = str(url)\n",
    "        \n",
    "        data.append((lang, family, dialects, iso6393, glottocode, speakers, regions, off_lang, rec_min_lang, url))\n",
    "    # except Exception as e: \n",
    "    #     errors.append((e, url))\n",
    "    #     continue\n",
    "\n",
    "df = pd.DataFrame(data, columns=['lang', 'family', 'dialects', 'iso6393', 'glottocode', 'speakers', 'regions', 'off_lang', 'rec_min_lang', 'Wikipedia_Url'])\n",
    "df.to_csv('../csv_files/wiki_languages_most_recent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['lang', 'family', 'dialects', 'iso6393', 'glottocode', 'speakers', 'regions', 'off_lang', 'rec_min_lang', 'Wikipedia_Url'])\n",
    "df.to_csv('../csv_files/wiki_languages_most_recent.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
